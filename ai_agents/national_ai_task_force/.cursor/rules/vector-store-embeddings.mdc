---
description: Standards for FAISS vector stores and embedding models in the National AI Task Force project
globs: *.py
alwaysApply: false
---
# FAISS Vector Store and Embedding Models

<rule>
name: vector_store_embeddings
description: Standards for implementing and using FAISS vector stores and embedding models in the project

filters:
  # Match Python files
  - type: file_extension
    pattern: "\\.py$"
  # Match files that use vector stores or embeddings
  - type: content
    pattern: "(?s)(vector_?store|embedding|FAISS|HuggingFaceEmbeddings)"

actions:
- type: suggest
  message: |
  ## Vector Store Standards
  
  The National AI Task Force project uses the following vector store standards:
  
  1. **Primary Vector Store**: `FAISS` from LangChain Community
     - High-performance vector similarity search library
     - Persistent storage with save/load functionality
     - Optimized for fast retrieval operations
     - Average query time: ~0.08-0.10 seconds for typical queries
  
  2. **Document Processing**:
     - Chunk size: 1000 characters
     - Chunk overlap: 200 characters
     - Use `RecursiveCharacterTextSplitter` for document splitting
     - Typical processing time: ~20-25 seconds for initial creation
  
  3. **Storage Configuration**:
     - Index path: `national_ai_task_force_faiss` in project root
     - Load time: ~0.05 seconds for existing index
     - Save time: ~0.01 seconds after creation
  
  ## Embedding Models
  
  The project uses the following embedding models:
  
  1. **Primary Embedding Model**: `sentence-transformers/all-mpnet-base-v2`
     - High-quality general-purpose embeddings
     - 768-dimensional vectors
     - Used via `HuggingFaceEmbeddings` from LangChain
     - Loading time: ~4-5 seconds on first initialization
  
  ## Performance Metrics
  
  The FAISS implementation provides significant performance improvements:
  
  1. **Query Performance**:
     - Average search time: 0.08-0.10 seconds
     - Consistent performance regardless of vector store size
     - 10-20x faster than naive similarity search methods
  
  2. **Memory Efficiency**:
     - Reduced memory footprint compared to in-memory solutions
     - Efficient index structure for large document collections
     - Scales well with increasing document count
  
  3. **Persistence Benefits**:
     - One-time creation cost (~20-25 seconds)
     - Fast loading of pre-built index (~0.05 seconds)
     - 400-500x speedup for subsequent application starts
  
  ## Implementation Guidelines
  
  When working with FAISS vector stores and embeddings:
  
  1. **Initialization Pattern**:
     - Always check for existing index before creating a new one
     - Use try/except blocks for robust error handling
     - Implement performance timing for operations
     - Log detailed information about each step
  
  2. **Resource Management**:
     - Initialize the vector store only once and reuse it
     - Use the singleton pattern to avoid reloading
     - Properly handle resources to prevent memory leaks
  
  3. **Error Handling**:
     - Add comprehensive error handling for all operations
     - Implement fallback mechanisms for critical failures
     - Log detailed error information for debugging
  
  4. **Optimization Techniques**:
     - Use appropriate FAISS index types for your use case
     - Consider batch processing for large document sets
     - Monitor and log performance metrics

examples:
- input: |
  # Example of correct FAISS vector store implementation
  
  import time
  import os
  from langchain_huggingface import HuggingFaceEmbeddings
  from langchain_community.vectorstores import FAISS
  from langchain.schema import Document
  
  # Constants
  CHUNK_SIZE = 1000
  CHUNK_OVERLAP = 200
  EMBEDDING_MODEL = "sentence-transformers/all-mpnet-base-v2"
  FAISS_INDEX_PATH = "national_ai_task_force_faiss"
  
  # Initialize embeddings
  embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
  
  # Check for existing index or create new one
  if os.path.exists(FAISS_INDEX_PATH):
      # Load existing index
      start_time = time.time()
      vector_store = FAISS.load_local(
          FAISS_INDEX_PATH,
          embeddings,
          allow_dangerous_deserialization=True
      )
      load_time = time.time() - start_time
      print(f"Loaded existing index in {load_time:.4f} seconds")
  else:
      # Create new index
      start_time = time.time()
      documents = load_documents()  # Your document loading function
      vector_store = FAISS.from_documents(documents, embeddings)
      create_time = time.time() - start_time
      
      # Save the index
      save_start = time.time()
      vector_store.save_local(FAISS_INDEX_PATH)
      save_time = time.time() - save_start
      
      print(f"Created index in {create_time:.4f} seconds")
      print(f"Saved index in {save_time:.4f} seconds")
  
  # Perform optimized search
  query = "What are the key recommendations?"
  search_start = time.time()
  results = vector_store.similarity_search_with_score(query, k=5)
  search_time = time.time() - search_start
  print(f"Search completed in {search_time:.4f} seconds")
  
  output: "Correctly implemented FAISS vector store with performance monitoring"

- input: |
  # Example of singleton pattern for FAISS vector store
  
  class VectorStoreManager:
      """
      Singleton manager for FAISS vector store to ensure
      it's only initialized once across the application.
      """
      _instance = None
      _vector_store = None
      
      def __new__(cls):
          if cls._instance is None:
              cls._instance = super(VectorStoreManager, cls).__new__(cls)
              cls._instance._initialize_vector_store()
          return cls._instance
      
      def _initialize_vector_store(self):
          """Initialize the FAISS vector store with proper error handling."""
          try:
              # Initialize embeddings
              self.embeddings = HuggingFaceEmbeddings(
                  model_name=EMBEDDING_MODEL
              )
              
              # Check for existing index
              if os.path.exists(FAISS_INDEX_PATH):
                  start_time = time.time()
                  self._vector_store = FAISS.load_local(
                      FAISS_INDEX_PATH,
                      self.embeddings,
                      allow_dangerous_deserialization=True
                  )
                  load_time = time.time() - start_time
                  logger.info(f"Loaded FAISS index in {load_time:.4f} seconds")
              else:
                  # Create new index if not exists
                  logger.info("Creating new FAISS index")
                  documents = self._load_documents()
                  start_time = time.time()
                  self._vector_store = FAISS.from_documents(
                      documents, 
                      self.embeddings
                  )
                  create_time = time.time() - start_time
                  logger.info(f"Created FAISS index in {create_time:.4f} seconds")
                  
                  # Save the index
                  self._vector_store.save_local(FAISS_INDEX_PATH)
                  logger.info(f"Saved FAISS index to {FAISS_INDEX_PATH}")
          except Exception as e:
              logger.error(f"Error initializing vector store: {e}")
              raise
      
      def get_vector_store(self):
          """Get the initialized vector store instance."""
          if self._vector_store is None:
              raise ValueError("Vector store not initialized")
          return self._vector_store
      
      def search(self, query, k=5):
          """
          Perform optimized search with performance monitoring.
          
          Args:
              query: The search query
              k: Number of results to return
              
          Returns:
              List of search results with scores
          """
          if self._vector_store is None:
              raise ValueError("Vector store not initialized")
              
          try:
              start_time = time.time()
              results = self._vector_store.similarity_search_with_score(query, k=k)
              search_time = time.time() - start_time
              logger.info(f"Search completed in {search_time:.4f} seconds")
              return results
          except Exception as e:
              logger.error(f"Search error: {e}")
              raise
  
  # Usage
  vector_store_manager = VectorStoreManager()
  results = vector_store_manager.search("What are the key recommendations?")
  
  output: "Correctly implemented singleton pattern for FAISS vector store with performance monitoring"

metadata:
  priority: high
  version: 2.0
</rule> 